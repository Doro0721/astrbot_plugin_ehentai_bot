from astrbot.api.event import filter, AstrMessageEvent, MessageEventResult
from astrbot.api.star import Context, Star, register
from astrbot.api.message_components import Image, Plain, Nodes, Node
from .utils.downloader import Downloader
from .utils.html_parser import HTMLParser
from .utils.message_adapter import MessageAdapter
from .utils.tag_translator import TagTranslator
from pathlib import Path
import os
import io
import re
import json
import aiohttp
import asyncio
import glob
import logging
import traceback
import tempfile
import base64 # æ–°å¢ base64
from typing import List, Optional, Dict, Any, Union
from urllib.parse import urlparse
from PIL import Image as PILImage, ImageDraw, ImageFont
import re # ç¡®ä¿ re è¢«å¯¼å…¥
from bs4 import BeautifulSoup # å¯¼å…¥ BeautifulSoup
try:
    from aiohttp_socks import ProxyConnector
    HAS_SOCKS = True
except ImportError:
    HAS_SOCKS = False

logger = logging.getLogger(__name__)


@register("astrbot_plugin_ehentai_bot", "Doro0721", "é€‚é… AstrBot çš„ EHentaiç”»å»Š è½¬ PDF æ’ä»¶", "4.2.7")
class EHentaiBot(Star):
    @staticmethod
    def _parse_proxy_config(proxy_str: str) -> Dict[str, Any]:
        """è§£æä»£ç†é…ç½®å­—ç¬¦ä¸²"""
        if not proxy_str:
            return {}
        
        parsed = urlparse(proxy_str)
        
        if parsed.scheme not in ('http', 'https', 'socks5'):
            raise ValueError("ä»…æ”¯æŒHTTP/HTTPS/SOCKS5ä»£ç†åè®®")
        
        auth = None
        if parsed.username and parsed.password:
            auth = aiohttp.BasicAuth(parsed.username, parsed.password)
        
        if not parsed.hostname:
            logger.warning(f"ä»£ç†é…ç½® '{proxy_str}' è§£æå¤±è´¥ï¼šæœªæ‰¾åˆ°ä¸»æœºåã€‚å·²å¿½ç•¥ä»£ç†è®¾ç½®ã€‚")
            return {}
            
        proxy_url = f"{parsed.scheme}://{parsed.hostname}"
        if parsed.port:
            proxy_url += f":{parsed.port}"
        
        return {
            'url': proxy_url,
            'auth': auth
        }
    
    @staticmethod
    def _transform_config(config: dict) -> Dict[str, Any]:
        """å°†æ‰å¹³é…ç½®è½¬æ¢ä¸ºåµŒå¥—å­—å…¸ç»“æ„"""
        # å¦‚æœå·²ç»æ˜¯åµŒå¥—ç»“æ„ï¼Œç›´æ¥è¿”å›
        if any(isinstance(v, dict) for v in config.values()):
            return config
        
        # é…ç½®æ˜ å°„è¡¨ï¼šæ‰å¹³é”® -> åµŒå¥—è·¯å¾„
        json_to_yaml_mapping = {
            "platform_type": ["platform", "type"],
            "platform_http_host": ["platform", "http_host"],
            "platform_http_port": ["platform", "http_port"],
            "platform_api_token": ["platform", "api_token"],
            "platform_use_base64_upload": ["platform", "use_base64_upload"],
            "request_headers_user_agent": ["request", "headers", "User-Agent"],
            "request_website": ["request", "website"],
            "request_cookies_ipb_member_id": ["request", "cookies", "ipb_member_id"],
            "request_cookies_ipb_pass_hash": ["request", "cookies", "ipb_pass_hash"],
            "request_cookies_igneous": ["request", "cookies", "igneous"],
            "request_cookies_sk": ["request", "cookies", "sk"],
            "request_proxies": ["request", "proxies"],
            "request_concurrency": ["request", "concurrency"],
            "request_max_retries": ["request", "max_retries"],
            "request_timeout": ["request", "timeout"],
            "output_image_folder": ["output", "image_folder"],
            "output_pdf_folder": ["output", "pdf_folder"],
            "output_search_cache_folder": ["output", "search_cache_folder"],
            "output_jpeg_quality": ["output", "jpeg_quality"],
            "output_max_pages_per_pdf": ["output", "max_pages_per_pdf"],
            "output_max_filename_length": ["output", "max_filename_length"],
            "features_enable_formatted_message_search": ["features", "enable_formatted_message_search"],
            "features_enable_cover_image_download": ["features", "enable_cover_image_download"],
        }
        
        # éœ€è¦ç±»å‹è½¬æ¢çš„å­—æ®µ
        int_fields = [
            "platform_http_port",
            "request_concurrency",
            "request_max_retries",
            "request_timeout",
            "output_jpeg_quality",
            "output_max_pages_per_pdf",
            "output_max_filename_length"
        ]
        
        bool_fields = [
            "platform_use_base64_upload",
            "features_enable_formatted_message_search",
            "features_enable_cover_image_download"
        ]
        
        # å¤„ç†é…ç½®å€¼
        processed_config = {}
        for key, value in config.items():
            if value == "" or value is None:
                continue
            
            if key in int_fields:
                try:
                    processed_config[key] = int(value)
                except (ValueError, TypeError):
                    logger.warning(f"æ— æ³•å°† {key} çš„å€¼ '{value}' è½¬æ¢ä¸ºæ•´æ•°ï¼Œå·²è·³è¿‡æ­¤é¡¹")
                    continue
            elif key in bool_fields:
                if isinstance(value, str):
                    processed_config[key] = value.lower() in ('true', '1', 'yes', 'on')
                else:
                    processed_config[key] = bool(value)
            else:
                processed_config[key] = value
        
        # è½¬æ¢ä¸ºåµŒå¥—ç»“æ„
        nested_config = {}
        for json_key, value in processed_config.items():
            if json_key in json_to_yaml_mapping:
                path_parts = json_to_yaml_mapping[json_key]
                current = nested_config
                for i, part in enumerate(path_parts[:-1]):
                    current = current.setdefault(part, {})
                current[path_parts[-1]] = value
        
        # åå¤„ç†ï¼šæ·»åŠ ä»£ç†é…ç½®å’ŒéªŒè¯cookies
        if 'request' in nested_config:
            request = nested_config['request']
            website = request.get('website')
            cookies = request.get('cookies', {})
            
            # å¦‚æœè®¾ç½®ä¸ºexhentaiä½†cookiesä¸å®Œæ•´ï¼Œåˆ‡æ¢ä¸ºe-hentai
            if website == 'exhentai':
                if any(not cookies.get(key, '') for key in ["ipb_member_id", "ipb_pass_hash", "igneous"]):
                    request['website'] = 'e-hentai'
                    logger.warning("ç½‘ç«™è®¾ç½®ä¸ºé‡Œç«™exhentaiä½†cookiesä¸å®Œæ•´ï¼Œå·²æ›´æ¢ä¸ºè¡¨ç«™e-hentai")
            
            # è§£æä»£ç†é…ç½®
            proxy_str = request.get('proxies', '')
            request['proxy_str'] = proxy_str # ä¿ç•™åŸå§‹å­—ç¬¦ä¸²
            proxy_config = EHentaiBot._parse_proxy_config(proxy_str)
            request['proxy'] = proxy_config
        
        # ç¡®ä¿å…³é”®é…ç½®é¡¹å§‹ç»ˆå­˜åœ¨é»˜è®¤ç»“æ„
        if 'output' not in nested_config:
            nested_config['output'] = {}
        if 'request' not in nested_config:
            nested_config['request'] = {}
        if 'features' not in nested_config:
            nested_config['features'] = {}
        if 'platform' not in nested_config:
            nested_config['platform'] = {}
        
        return nested_config
    
    def __init__(self, context: Context, config: dict):
        super().__init__(context)
        self.config = self._transform_config(config)
        self.parser = HTMLParser()
        self.uploader = MessageAdapter(self.config)
        self.downloader = Downloader(self.config, self.uploader, self.parser)
        self.tag_translator = TagTranslator()

    def add_number_to_image(self, image: PILImage.Image, number: int) -> PILImage.Image:
        """ä¸ºå•å¼ å›¾ç‰‡æ·»åŠ æ•°å­—åºå·"""
        image = image.convert("RGBA")
        txt_layer = PILImage.new("RGBA", image.size, (255, 255, 255, 0))
        draw = ImageDraw.Draw(txt_layer)

        try:
            font = ImageFont.truetype("msyh.ttc", size=60)
        except IOError:
            try:
                font = ImageFont.truetype("arial.ttf", size=60)
            except IOError:
                font = ImageFont.load_default()

        text = str(number)

        bbox = draw.textbbox((0, 0), text, font=font)
        text_width = bbox[2] - bbox[0]
        text_height = bbox[3] - bbox[1]

        rect_height = text_height + 20
        rect_pos = (0, image.height - rect_height, image.width, image.height)
        draw.rectangle(rect_pos, fill=(0, 0, 0, 150))

        text_x = (image.width - text_width) / 2
        text_y = image.height - rect_height + 10
        draw.text((text_x, text_y), text, font=font, fill=(255, 255, 255, 255))

        out = PILImage.alpha_composite(image, txt_layer)
        return out.convert("RGB")

    @staticmethod
    def split_text_by_length(text: str, max_length: int = 4000) -> List[str]:
        result = []
        label = 'ç”»å»Šé“¾æ¥'
        start = 0
        last_link_end = -1
        last_newline = -1
        for i, ch in enumerate(text):
            if ch == '\n':
                last_newline = i
            if text.startswith(label, i - len(label) + 1):
                next_newline_pos = text.find('\n', i)
                if next_newline_pos != -1:
                    last_link_end = next_newline_pos + 1
                else:
                    last_link_end = len(text)
            if i - start + 1 >= max_length:
                cut = last_link_end if last_link_end > start else (
                    last_newline + 1 if last_newline >= start else start + max_length)
                result.append(text[start:cut])
                start = cut
                last_link_end = -1
                last_newline = -1
        if start < len(text):
            result.append(text[start:])
        return result

    async def _resolve_url_from_input(self, event: AstrMessageEvent, user_input: str) -> Optional[str]:
        """ä»ç”¨æˆ·è¾“å…¥ï¼ˆURLæˆ–åºå·ï¼‰è§£æç”»å»ŠURL"""
        output_config = self.config.get('output', {})
        search_cache_folder = Path(output_config.get('search_cache_folder', 'data/ehentai/searchCache'))
        pattern = re.compile(r'^https://(e-hentai|exhentai)\.org/g/\d{7}/[a-f0-9]{10}/?$')

        if pattern.match(user_input):
            return user_input

        if user_input.isdigit() and int(user_input) > 0:
            cache_file = search_cache_folder / f"{event.get_sender_id()}.json"
            if not cache_file.exists():
                await event.send(event.plain_result("æœªæ‰¾åˆ°æœç´¢è®°å½•ï¼Œè¯·å…ˆä½¿ç”¨'æœeh'å‘½ä»¤"))
                return None

            with open(cache_file, 'r', encoding='utf-8') as f:
                cache_data = json.load(f)

            if user_input in cache_data:
                url = cache_data[user_input]
                await event.send(event.plain_result(f"æ­£åœ¨è·å–ç”»å»Šé“¾æ¥: {url}"))
                return url
            else:
                await event.send(event.plain_result(f"æœªæ‰¾åˆ°ç´¢å¼•ä¸º {user_input} çš„ç”»å»Š"))
                return None

        await event.send(event.plain_result("è¾“å…¥çš„ç”»å»Šé“¾æ¥æˆ–åºå·æ— æ•ˆï¼Œè¯·é‡è¯•..."))
        return None

    @staticmethod
    def parse_command(message: str) -> List[str]:
        cleaned_text = re.sub(r'@\S+\s*', '', message).strip()
        return [p for p in cleaned_text.split(' ') if p][1:]

    async def _get_session(self) -> aiohttp.ClientSession:
        """æ ¹æ®é…ç½®åˆ›å»ºä¸€ä¸ªå¸¦æœ‰æ­£ç¡®ä»£ç†è®¾ç½®çš„ aiohttp.ClientSession"""
        request_config = self.config.get('request', {})
        proxy_str = request_config.get('proxy_str', '')
        
        connector = None
        if proxy_str and proxy_str.startswith('socks5'):
            if HAS_SOCKS:
                connector = ProxyConnector.from_url(proxy_str, ssl=False)
            else:
                logger.error("æ£€æµ‹åˆ° SOCKS5 ä»£ç†é…ç½®ï¼Œä½†æœªå®‰è£… aiohttp-socks åº“ã€‚è¯·è¿è¡Œ 'pip install aiohttp-socks'")
        
        if connector is None:
            connector = aiohttp.TCPConnector(ssl=False)
            
        return aiohttp.ClientSession(connector=connector)

    async def download_thumbnail(self, url: str, session: aiohttp.ClientSession, semaphore: asyncio.Semaphore):
        """ä¸‹è½½å°é¢å›¾ç‰‡"""
        try:
            # Prefer User-Agent from config, but keep image-specific headers
            headers = {
                'User-Agent': self.config.get('request', {}).get('headers', {}).get('User-Agent',
                                                                                    'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'),
                'Referer': f"https://{self.config.get('request', {}).get('website', 'e-hentai')}.org/",
                'Accept': 'image/webp,image/apng,image/*,*/*;q=0.8',
                'Accept-Language': 'zh-CN,zh;q=0.9,en;q=0.8'
            }

            request_config = self.config.get('request', {})
            proxy_conf = request_config.get('proxy', {})
            proxy_str = request_config.get('proxy_str', '')
            
            cookies = request_config.get('cookies') if request_config.get('website') == 'exhentai' else None
            timeout = aiohttp.ClientTimeout(total=request_config.get('timeout', 30))
            
            # åªæœ‰ http/https ä»£ç†ä½¿ç”¨ aiohttp åŸç”Ÿ proxy å‚æ•°
            proxy = None
            proxy_auth = None
            if not proxy_str.startswith('socks5'):
                proxy = proxy_conf.get('url')
                proxy_auth = proxy_conf.get('auth')

            async with semaphore:
                async with session.get(
                        url,
                        headers=headers,
                        cookies=cookies,
                        proxy=proxy,
                        proxy_auth=proxy_auth,
                        timeout=timeout,
                        ssl=False
                ) as response:
                    response.raise_for_status()
                    return PILImage.open(io.BytesIO(await response.read()))
        except Exception as e:
            logger.warning(f"ä¸‹è½½å°é¢å›¾ç‰‡å¤±è´¥: {url} - {e}")
            return None

    async def _download_thumbnail_with_tracking(self, url: str, session: aiohttp.ClientSession,
                                                semaphore: asyncio.Semaphore):
        """åŒ…è£…å°é¢ä¸‹è½½ä»»åŠ¡ä»¥è¿›è¡Œè·Ÿè¸ª"""
        image = await self.download_thumbnail(url, session, semaphore)
        if image:
            return {"success": True, "image": image, "url": url}
        else:
            return {"success": False, "error": "Download failed", "url": url}

    async def _download_covers_with_retry(self, search_results: List[dict]) -> List[PILImage.Image]:
        """å¸¦é‡è¯•æœºåˆ¶çš„å°é¢ä¸‹è½½å™¨"""
        if not self.config.get('features', {}).get('enable_cover_image_download', True):
            return []

        concurrency = self.config.get('request', {}).get('concurrency', 5)
        semaphore = asyncio.Semaphore(concurrency)

        urls_to_download = [res['cover_url'] for res in search_results if res.get('cover_url')]
        if not urls_to_download:
            return []

        async with await self._get_session() as session:
            # é¦–æ¬¡å°è¯•
            tasks = [self._download_thumbnail_with_tracking(url, session, semaphore) for url in urls_to_download]
            results = await asyncio.gather(*tasks)

            successful_images = [r['image'] for r in results if r.get('success')]
            failed_urls = [r['url'] for r in results if not r.get('success')]

            # é‡è¯•é€»è¾‘
            if failed_urls:
                logger.info(f"é¦–æ¬¡å°é¢ä¸‹è½½æœ‰ {len(failed_urls)} å¼ å¤±è´¥ï¼Œæ­£åœ¨é‡è¯•...")
                await asyncio.sleep(1)  # é‡è¯•å‰çŸ­æš‚å»¶è¿Ÿ

                retry_tasks = [self._download_thumbnail_with_tracking(url, session, semaphore) for url in failed_urls]
                retry_results = await asyncio.gather(*retry_tasks)

                successful_images.extend([r['image'] for r in retry_results if r.get('success')])
                final_failed_count = sum(1 for r in retry_results if not r.get('success'))

                if final_failed_count > 0:
                    logger.warning(f"å°é¢ä¸‹è½½é‡è¯•åä»æœ‰ {final_failed_count} å¼ å¤±è´¥ã€‚")

        return successful_images

    def create_combined_image(self, images):
        """å°†å¤šä¸ªå°é¢å›¾ç‰‡æ‹¼æ¥æˆä¸€å¼ å›¾ç‰‡ï¼ŒæŒ‰äº”å¼ ä¸€æ’æ’åˆ—"""
        if not images:
            return None

        valid_images = [img for img in images if img is not None]
        if not valid_images:
            return None

        # ä¸ºæ¯å¼ å›¾ç‰‡æ·»åŠ ç¼–å·
        numbered_images = [self.add_number_to_image(img, i) for i, img in enumerate(valid_images, 1)]

        target_height = 800
        padding = 10
        images_per_row = 5

        scaled_widths = []
        for img in numbered_images:
            width, height = img.size
            scaled_width = int((width * target_height) / height)
            scaled_widths.append(scaled_width)

        rows = []
        current_row_widths = []
        current_row_total = 0

        for i, scaled_width in enumerate(scaled_widths):
            if len(current_row_widths) < images_per_row:
                current_row_widths.append(scaled_width)
                current_row_total += scaled_width
            else:
                rows.append((current_row_widths, current_row_total))
                current_row_widths = [scaled_width]
                current_row_total = scaled_width

        if current_row_widths:
            rows.append((current_row_widths, current_row_total))

        max_row_width = max(row_total for _, row_total in rows) if rows else 0
        total_width = max_row_width + (images_per_row - 1) * padding

        total_height = len(rows) * target_height + (len(rows) - 1) * padding

        combined_image = PILImage.new('RGB', (total_width, total_height), (255, 255, 255))

        y_offset = 0
        image_index = 0
        for row_widths, row_total in rows:
            row_start_x = (total_width - (row_total + (len(row_widths) - 1) * padding)) // 2
            x_offset = row_start_x

            for scaled_width in row_widths:
                img = numbered_images[image_index]
                img = img.convert('RGB')
                img = img.resize((scaled_width, target_height), PILImage.Resampling.LANCZOS)

                combined_image.paste(img, (x_offset, y_offset))
                x_offset += scaled_width + padding
                image_index += 1

            y_offset += target_height + padding

        self.add_random_blocks(combined_image)
        return combined_image

    def add_random_blocks(self, image):
        """æ·»åŠ éšæœºè‰²å—å¹¶è¿›è¡Œè½»å¾®å›¾åƒå˜æ¢ä»¥è§„é¿å›¾ç‰‡å®¡æŸ¥"""
        import random
        from PIL import ImageOps, ImageEnhance

        # 1. éšæœºæ°´å¹³ç¿»è½¬ (æå…¶æœ‰æ•ˆçš„ Hash è§„é¿)
        if random.random() > 0.5:
            image = ImageOps.mirror(image)

        # 2. è½»å¾®äº®åº¦è°ƒèŠ‚ (æ”¹å˜åƒç´ å€¼)
        enhancer = ImageEnhance.Brightness(image)
        image = enhancer.enhance(random.uniform(0.98, 1.02))

        width, height = image.size
        
        # 3. æ·»åŠ å°‘é‡æå°éšæœºè‰²å—
        num_blocks = random.randint(5, 10)
        for _ in range(num_blocks):
            x1 = random.randint(0, width - 1)
            y1 = random.randint(0, height - 1)
            block_width = random.randint(1, 3)
            block_height = random.randint(1, 3)
            x2 = min(x1 + block_width, width - 1)
            y2 = min(y1 + block_height, height - 1)
            
            r, g, b = random.randint(0, 255), random.randint(0, 255), random.randint(0, 255)
            alpha = random.randint(10, 30)
            
            for x in range(x1, x2):
                for y in range(y1, y2):
                    current = image.getpixel((x, y))
                    new_r = int((current[0] * (255 - alpha) + r * alpha) / 255)
                    new_g = int((current[1] * (255 - alpha) + g * alpha) / 255)
                    new_b = int((current[2] * (255 - alpha) + b * alpha) / 255)
                    image.putpixel((x, y), (new_r, new_g, new_b))
                    
        return image


    @filter.command("es")
    async def handle_es(self, event: AstrMessageEvent):
        """
        æœç´¢ EHentai ç”»å»Š
        ç”¨æ³•: /es <å…³é”®è¯> [é¡µç ]
        ç¤ºä¾‹: /es loli
        ç¤ºä¾‹: /es loli 2
        """
        # è§£æå‚æ•°ï¼šç±»ä¼¼äº nhentai çš„è§£æé€»è¾‘
        message = event.message_str.strip()
        parts = message.split(maxsplit=1)
        
        if len(parts) < 2:
            yield event.plain_result(
                "ğŸ” EHentai æœç´¢\n"
                "ç”¨æ³•: /es <å…³é”®è¯> [é¡µç ]\n"
                "ç¤ºä¾‹: /es loli 2"
            )
            return

        query_str = parts[1].strip()
        words = query_str.split()
        
        # æ£€æŸ¥æœ€åä¸€ä¸ªè¯æ˜¯å¦ä¸ºé¡µç 
        target_page = 1
        if len(words) > 1 and words[-1].isdigit():
            target_page = int(words[-1])
            query = " ".join(words[:-1])
        else:
            query = query_str
            
        await self._search_and_reply(event, query, target_page)

    async def _search_and_reply(self, event: AstrMessageEvent, query: str, page: int):
        """æ‰§è¡Œæœç´¢å¹¶å›å¤ç»“æœï¼ˆä¾› /es å’Œç¿»é¡µä½¿ç”¨ï¼‰"""
        # å‘é€æç¤º
        yield event.plain_result(f"ğŸ” æ­£åœ¨æœç´¢: {query} (ç¬¬{page}é¡µ)...")

        try:
            search_results = await self.downloader.crawl_ehentai(
                query,
                0, # min_rating
                0, # min_pages
                page - 1 # target_page
            )

            if not search_results:
                yield event.plain_result("æœªæ‰¾åˆ°ç¬¦åˆæ¡ä»¶çš„ç»“æœ")
                return

            # ç¼“å­˜æœç´¢ç»“æœï¼ˆç”¨äºå¿«é€Ÿä¸‹è½½å’Œç¿»é¡µï¼‰
            user_id = event.get_sender_id()
            cache_data = {
                "results": search_results,
                "time": asyncio.get_event_loop().time(),
                "query": query,
                "page": page
            }
            
            if not hasattr(self, '_search_cache'):
                self._search_cache = {}
            self._search_cache[user_id] = cache_data

            # æ„å»ºæ¶ˆæ¯é“¾
            chain = []
            header = f"ğŸ” æœç´¢ç»“æœ (ç¬¬ {page} é¡µ)\nâ”â”â”â”â”â”â”â”â”â”â”â”\n"
            chain.append(Plain(header))

            # å¼‚æ­¥ä¸‹è½½æ‰€æœ‰å°é¢
            semaphore = asyncio.Semaphore(5)
            
            # å¤ç”¨ _download_covers_with_retry
            covers = await self._download_covers_with_retry(search_results)

            for idx, result in enumerate(search_results, 1):
                # æ–‡æœ¬éƒ¨åˆ†
                title = result['title']
                
                # å°è¯•ä» gallery_url æå– gid/token
                g_url = result['gallery_url']
                g_parts = g_url.strip('/').split('/')
                if len(g_parts) >= 2:
                    current_gid = g_parts[-2]
                    current_token = g_parts[-1]
                else:
                    current_gid = "?"
                    current_token = "?"
                
                # æ›´æ–° result ä»¥åŒ…å« gid (ç”¨äºå¿«é€Ÿä¸‹è½½)
                result['_gid'] = current_gid
                result['_token'] = current_token

                info = f"[{idx}] ğŸ“– {title}\n"
                info += f"ğŸ”– ID: {current_gid} | ğŸ“„ {result['pages']}é¡µ | â­ {result['rating']}\n"
                info += f"âœï¸ ä½œè€…: {result['author']} | ğŸ“‚ {result['category']}\n"
                info += f"ğŸ“… {result['timestamp']}\n"
                
                chain.append(Plain(info))

                # å›¾ç‰‡éƒ¨åˆ†
                if idx <= len(covers) and covers[idx-1]:
                    img = covers[idx-1]
                    buffered = io.BytesIO()
                    img.save(buffered, format="JPEG")
                    img_b64 = base64.b64encode(buffered.getvalue()).decode('utf-8')
                    chain.append(Image.fromBase64(img_b64))
                
                chain.append(Plain("\nâ”â”â”â”â”â”â”â”â”â”â”â”\n" if idx < len(search_results) else "\n"))

            footer = "\nğŸ’¡ 30ç§’å†…å›å¤:\nâ€¢ æ•°å­—(1-9): ä¸‹è½½å¯¹åº”ç”»å»Š\nâ€¢ 'ä¸‹': ä¸‹ä¸€é¡µ | 'ä¸Š': ä¸Šä¸€é¡µ"
            chain.append(Plain(footer))
            
            yield event.chain_result(chain)

        except Exception as e:
            logger.exception("æœç´¢å¤„ç†å¼‚å¸¸")
            yield event.plain_result(f"æœç´¢å‡ºé”™: {str(e)}")

    @filter.regex(r"^(?:\d+|ä¸Š|ä¸‹)$")
    async def handle_quick_interaction(self, event: AstrMessageEvent):
        """å¤„ç†å¿«é€Ÿäº¤äº’ï¼šæ•°å­—ä¸‹è½½ã€ç¿»é¡µ"""
        text = event.message_str.strip()
        user_id = event.get_sender_id()
        
        # æ£€æŸ¥ç¼“å­˜
        if not hasattr(self, '_search_cache') or user_id not in self._search_cache:
            return 
            
        cache = self._search_cache[user_id]
        # æ£€æŸ¥è¿‡æœŸ (30ç§’)
        if asyncio.get_event_loop().time() - cache["time"] > 30:
            del self._search_cache[user_id]
            return 
            
        # å¤„ç†ç¿»é¡µ
        if text == "ä¸Š":
            current_page = cache.get("page", 1)
            new_page = current_page - 1
            if new_page < 1:
                yield event.plain_result("ğŸš« å·²ç»æ˜¯ç¬¬ä¸€é¡µäº†")
                return
            
            # æ›´æ–°ç¼“å­˜æ—¶é—´é˜²æ­¢è¿‡æœŸï¼Œè™½ç„¶ _search_and_reply ä¼šè¦†ç›–
            async for result in self._search_and_reply(event, cache["query"], new_page):
                yield result
            return

        elif text == "ä¸‹":
            current_page = cache.get("page", 1)
            new_page = current_page + 1
            
            async for result in self._search_and_reply(event, cache["query"], new_page):
                yield result
            return

        # å¤„ç†ä¸‹è½½ (çº¯æ•°å­—)
        if not text.isdigit():
            return

        idx = int(text)
        results = cache["results"]
        if idx < 1 or idx > len(results):
            return 
            
        target = results[idx-1]
        gid = target.get('_gid')
        token = target.get('_token')
        
        if not gid or not token:
            yield event.plain_result("æ— æ³•è§£æç”»å»Šä¿¡æ¯ï¼Œè¯·é‡æ–°æœç´¢")
            return
            
        # è§¦å‘ä¸‹è½½æµç¨‹
        yield event.plain_result(f"ğŸš€ å·²é€‰æ‹© [{idx}]ï¼Œå¼€å§‹ä¸‹è½½ ID: {gid}...")
        
        # æ¸…é™¤ç¼“å­˜é˜²æ­¢é‡å¤è§¦å‘
        del self._search_cache[user_id]
        
        # è°ƒç”¨ä¸‹è½½é€»è¾‘
        await self.download_gallery(event, gid, token)


    async def send_formatted_search_results(self, event, result_text, search_results, combined_image_obj=None):
        """å‘é€æ ¼å¼åŒ–æœç´¢ç»“æœï¼ˆè½¬å‘æ¶ˆæ¯æ ¼å¼ï¼‰"""
        text_parts = self.split_text_by_length(result_text)
        sender_name = "å›¾ç‰‡æœç´¢bot"
        sender_id = event.get_self_id()
        try:
            sender_id = int(sender_id)
        except Exception:
            pass

        nodes_list = []
        temp_file_path = ''
        try:
            if combined_image_obj:
                self.add_random_blocks(combined_image_obj)

                img_byte_arr = io.BytesIO()
                combined_image_obj.save(img_byte_arr, 'JPEG', quality=85)

                with tempfile.NamedTemporaryFile(delete=False, suffix=".jpg") as temp_file:
                    temp_file.write(img_byte_arr.getvalue())
                    temp_file_path = temp_file.name

                image_node = Node(
                    name=sender_name,
                    uin=sender_id,
                    content=[Image(temp_file_path)]
                )
                nodes_list.append(image_node)

            for i, part in enumerate(text_parts):
                text_node = Node(
                    name=sender_name,
                    uin=sender_id,
                    content=[Plain(f"[  æœç´¢ç»“æœ {i + 1} / {len(text_parts)}  ]\n\n{part}")]
                )
                nodes_list.append(text_node)

            if nodes_list:
                nodes = Nodes(nodes_list)
                await event.send(event.chain_result([nodes]))
        finally:
            if temp_file_path and os.path.exists(temp_file_path):
                os.unlink(temp_file_path)

    # @filter.command("ehç¿»é¡µ")
    # async def jump_to_page(self, event: AstrMessageEvent):
    #     pass
        

    async def download_gallery(self, event: AstrMessageEvent, gid: str = None, token: str = None):
        """ä¸‹è½½ç”»å»Šï¼ˆæ”¯æŒç›´æ¥è°ƒç”¨æˆ–å‘½ä»¤è°ƒç”¨ï¼‰"""
        output_config = self.config.get('output', {})
        image_folder = Path(output_config.get('image_folder', 'data/ehentai/tempImages'))
        image_folder.mkdir(exist_ok=True, parents=True) # ä½¿ç”¨ç»å¯¹è·¯å¾„? main.py é‡Œæ²¡æœ‰ self.image_folder å­˜å‚¨ç»å¯¹è·¯å¾„ï¼Œæ˜¯åœ¨ Downloader é‡Œã€‚
        # è¿™é‡Œåªæ˜¯åˆ›å»ºç›®å½•ï¼ŒDownloader ä¼šå†æ¬¡å¤„ç†ã€‚
        
        # ä¿®æ­£ï¼šç§»é™¤ main.py é‡Œå¯¹ output_config çš„è·¯å¾„å¤„ç†ï¼Œç›´æ¥ä¾èµ– Downloader
        # æˆ–è€…ä¸ºäº†ä¿é™©èµ·è§ï¼Œè¿™é‡Œä¸å¤„ç†ç›®å½•ï¼Œåªè´Ÿè´£è§£æå‚æ•°ã€‚
        
        try:
            url = ""
            if gid and token:
                website = self.config.get('request', {}).get('website', 'e-hentai')
                url = f"https://{website}.org/g/{gid}/{token}/"
            else:
                args = self.parse_command(event.message_str)
                if len(args) != 1:
                    # å¦‚æœä¸æ˜¯å‘½ä»¤è°ƒç”¨ï¼Œæˆ–è€…æ˜¯å‚æ•°ä¸å¯¹
                     # ç”±äºç§»é™¤äº† help å‘½ä»¤ï¼Œè¿™é‡Œç›´æ¥è¿”å›æç¤º
                    await event.send(event.plain_result("å‚æ•°é”™è¯¯"))
                    return

                url = await self._resolve_url_from_input(event, args[0])
            
            if not url:
                return

            # è®°å½•æ—¥å¿—è€Œéå‘é€æ¶ˆæ¯
            logger.info(f"å¼€å§‹ä¸‹è½½: {url}")

            async with await self.downloader._get_session() as session:
                is_pdf_exist = await self.downloader.process_pagination(event, session, url)

                if not is_pdf_exist:
                    # ä½¿ç”¨ downloader çš„ stored gallery_title
                    title = self.downloader.gallery_title
                    safe_title = await self.downloader.merge_images_to_pdf(event, title)
                    # output_config é‡Œçš„ pdf_folder å¯èƒ½æ˜¯ç›¸å¯¹è·¯å¾„ï¼ŒDownloader é‡Œæ˜¯ç»å¯¹è·¯å¾„ã€‚
                    # upload_file éœ€è¦ç»å¯¹è·¯å¾„ã€‚
                    # ä» downloader è·å–ç»å¯¹è·¯å¾„
                    pdf_folder = self.downloader.pdf_folder
                    await self.uploader.upload_file(event, pdf_folder, safe_title)

                    # å‘é€åè‡ªåŠ¨æ¸…ç† PDF æ–‡ä»¶
                    try:
                        pattern = re.compile(rf"^{re.escape(safe_title)}(?: part \d+)?\.pdf$")
                        for f in os.listdir(pdf_folder):
                            if pattern.match(f):
                                os.remove(os.path.join(pdf_folder, f))
                        logger.info(f"å·²æ¸…ç† PDF æ–‡ä»¶: {safe_title}")
                    except Exception as e:
                        logger.warning(f"æ¸…ç† PDF æ–‡ä»¶å¤±è´¥: {e}")

        except Exception as e:
            logger.exception("ä¸‹è½½å¤±è´¥")
            stack_info = traceback.format_exc()
            await event.send(event.plain_result(f"ä¸‹è½½å¤±è´¥ï¼š{str(e)}\n{stack_info}"))

    @filter.regex(r"https?://(?:e-hentai|exhentai)\.org/g/\d+/[a-f0-9]+/?")
    async def handle_link_parsing(self, event: AstrMessageEvent, *args):
        """è§£æ E-Hentai/ExHentai ç”»å»Šé“¾æ¥å¹¶æ˜¾ç¤ºå¡ç‰‡ä¿¡æ¯"""
        # å…¼å®¹æ€§å¤„ç†ï¼šå¦‚æœ event ä¸æ˜¯äº‹ä»¶å¯¹è±¡ï¼ˆå¯èƒ½æ˜¯å‚æ•°åç§»ï¼‰ï¼Œåˆ™ä»å‚æ•°ä¸­å¯»æ‰¾
        if not hasattr(event, "message_str"):
            for arg in args:
                if hasattr(arg, "message_str"):
                    event = arg
                    break
        
        if not hasattr(event, "message_str"):
            logger.error(f"æ— æ³•è·å–æ¶ˆæ¯å†…å®¹ï¼Œeventç±»å‹: {type(event)}")
            return

        text = event.message_str.strip()
        # æå–é“¾æ¥
        pattern = re.compile(r"https?://(e-hentai|exhentai)\.org/g/(\d+)/([a-f0-9]+)/?")
        match = pattern.search(text)
        if not match:
            return
            
        domain, gid, token = match.groups()
        url = match.group(0)
        
        await event.send(event.plain_result(f"ğŸ” æ­£åœ¨è§£æç”»å»Š: {gid} ..."))
        
        # ä¿å­˜åŸå§‹æ¶ˆæ¯IDï¼Œç”¨äºä¸‹è½½å®Œåè¡¨æƒ…å›åº”
        original_msg_id = None
        try:
            original_msg_id = event.message_obj.message_id
        except:
            pass
        
        try:
            # ä½¿ç”¨åŒä¸€ä¸ª session å®Œæˆ HTML è·å–å’Œå°é¢ä¸‹è½½
            async with await self._get_session() as session:
                # ç¡®ä¿æ ‡ç­¾ç¿»è¯‘æ•°æ®å·²åŠ è½½
                await self.tag_translator.ensure_loaded(session)
                
                html = await self.downloader.fetch_with_retry(session, url)
                
                if not html:
                    await event.send(event.plain_result("æ— æ³•è·å–ç”»å»Šè¯¦æƒ…"))
                    return
                    
                # ä½¿ç”¨ extract_gallery_info è·å–æ ‡é¢˜
                title, _ = self.parser.extract_gallery_info(html)
                
                soup = BeautifulSoup(html, "html.parser")
                
                # æ ‡é¢˜
                gn = soup.select_one("#gn")
                gj = soup.select_one("#gj")
                title_en = gn.text.strip() if gn else ""
                title_jp = gj.text.strip() if gj else ""
                
                if title_jp and title_en and title_jp != title_en:
                    display_title = f"{title_jp}\n{title_en}"
                else:
                    display_title = title_jp or title_en or title
                
                # æ ‡ç­¾æ˜ å°„è¡¨
                tag_mapping = {
                    "language": "è¯­è¨€",
                    "parody": "åŸä½œ",
                    "character": "è§’è‰²",
                    "group": "ç¤¾å›¢",
                    "artist": "è‰ºæœ¯å®¶",
                    "female": "å¥³æ€§",
                    "male": "ç”·æ€§",
                    "mixed": "æ··åˆ",
                    "other": "å…¶ä»–",
                    "misc": "å…¶ä»–"
                }
                
                # æ ‡ç­¾è§£æï¼ˆä½¿ç”¨ EhTagTranslation ç¿»è¯‘æ ‡ç­¾å€¼ï¼‰
                tag_rows = soup.select("#taglist tr")
                tags_text = ""
                for row in tag_rows:
                    tds = row.find_all("td")
                    if len(tds) == 2:
                        cat_raw = tds[0].text.strip(":")
                        cat_cn = tag_mapping.get(cat_raw, cat_raw)
                        
                        tag_links = tds[1].find_all("a")
                        tag_names = []
                        for t in tag_links:
                            raw_tag = t.text.strip().split(" | ")[0]
                            cn_tag = self.tag_translator.translate(cat_raw, raw_tag)
                            tag_names.append(f"#{cn_tag}")
                        
                        if tag_names:
                            tags_text += f"{cat_cn}: {' '.join(tag_names)}\n"

                # æ„å»ºæ¶ˆæ¯
                chain = []
                
                # æ ‡é¢˜ + æ ‡ç­¾åˆä¸ºä¸€æ®µ
                info_text = f"{display_title}\n"
                if tags_text:
                    info_text += tags_text
                chain.append(Plain(info_text))
                
                # è·å–ç”»å»Šç¬¬ä¸€å¼ åŸå›¾ä½œä¸ºé¢„è§ˆå°é¢
                cover_img_obj = None
                try:
                    subpage_urls = self.parser.extract_subpage_urls(html)
                    if subpage_urls:
                        first_page_html = await self.downloader.fetch_with_retry(session, subpage_urls[0])
                        if first_page_html:
                            sub_soup = BeautifulSoup(first_page_html, "html.parser")
                            first_img_url = sub_soup.select_one("#img")
                            if first_img_url:
                                first_img_url = first_img_url.get("src")
                            
                            if not first_img_url:
                                img_el = sub_soup.select_one("#i3 img")
                                if img_el:
                                    first_img_url = img_el.get("src")
                            
                            if first_img_url:
                                img_bytes = await self.downloader.fetch_bytes_with_retry(session, first_img_url)
                                if img_bytes:
                                    cover_img_obj = PILImage.open(io.BytesIO(img_bytes))
                except Exception as e:
                    logger.warning(f"è·å–ç¬¬ä¸€å¼ åŸå›¾å¤±è´¥ï¼Œå›é€€åˆ°ç¼©ç•¥å›¾: {e}")
                
                # å›é€€ï¼šå¦‚æœåŸå›¾è·å–å¤±è´¥ï¼Œä½¿ç”¨ç¼©ç•¥å›¾
                if not cover_img_obj:
                    cover_url = None
                    cover_img_tag = soup.select_one("#gd1 img")
                    if cover_img_tag:
                        cover_url = cover_img_tag.get("src")
                    if not cover_url:
                        cover_div = soup.select_one("#gd1 div")
                        if cover_div:
                            style = cover_div.get("style", "")
                            m = re.search(r'url\((.+?)\)', style)
                            if m:
                                cover_url = m.group(1).strip("'\"")
                    if not cover_url:
                        og_img = soup.select_one('meta[property="og:image"]')
                        if og_img:
                            cover_url = og_img.get("content")
                    if cover_url:
                        logger.info(f"å›é€€å°é¢ URL: {cover_url}")
                        semaphore = asyncio.Semaphore(1)
                        cover_img_obj = await self.download_thumbnail(cover_url, session, semaphore)
                
                # æ„å»ºå°é¢æ¶ˆæ¯
                if cover_img_obj:
                    try:
                        # é™ä½è§„æ ¼ä»¥ç¡®ä¿ QQ å‘é€æˆåŠŸç‡
                        max_side = 700
                        w, h = cover_img_obj.size
                        if max(w, h) > max_side:
                            ratio = max_side / max(w, h)
                            cover_img_obj = cover_img_obj.resize(
                                (int(w * ratio), int(h * ratio)),
                                PILImage.Resampling.LANCZOS
                            )
                        # åº”ç”¨åå’Œè°å¤„ç†
                        cover_img_obj = self.add_random_blocks(cover_img_obj)
                        buffered = io.BytesIO()
                        # è°ƒæ•´ JPEG è´¨é‡åˆ° 80ï¼Œå…¼é¡¾ä½“ç§¯å’Œæ¸…æ™°åº¦
                        cover_img_obj.convert("RGB").save(buffered, format="JPEG", quality=80)
                        img_b64 = base64.b64encode(buffered.getvalue()).decode('utf-8')
                        chain.append(Image.fromBase64(img_b64))
                    except Exception as e:
                        logger.error(f"å¤„ç†å°é¢å›¾å¤±è´¥: {e}")
                else:
                    logger.warning("æœªèƒ½è·å–å°é¢å›¾")
            
            # å‘é€è¯¦æƒ…ï¼ˆå¦‚æœå¸¦å›¾å‘é€å¤±è´¥ï¼Œå›é€€åˆ°çº¯æ–‡å­—ï¼‰
            try:
                await event.send(event.chain_result(chain))
            except Exception as e:
                logger.warning(f"å¸¦å›¾å‘é€å¤±è´¥ï¼ˆå¯èƒ½è¢«å’Œè°ï¼‰ï¼Œå›é€€çº¯æ–‡å­—: {e}")
                text_chain = [item for item in chain if not isinstance(item, Image)]
                if text_chain:
                    try:
                        await event.send(event.chain_result(text_chain))
                    except Exception:
                        pass
            
            # è‡ªåŠ¨ä¸‹è½½
            await self.download_gallery(event, gid, token)
            
            # ä¸‹è½½å®Œæˆåå¯¹åŸæ¶ˆæ¯æ·»åŠ è¡¨æƒ…å›åº”
            if original_msg_id:
                try:
                    await self.uploader.set_msg_emoji_like(str(original_msg_id), "66")  # 66=â¤ï¸çˆ±å¿ƒ
                except Exception as e:
                    logger.warning(f"è¡¨æƒ…å›åº”å¤±è´¥: {e}")
            
        except Exception as e:
            logger.error(f"é“¾æ¥è§£æå¤±è´¥: {e}")
            await event.send(event.plain_result(f"è§£æå¤±è´¥: {e}"))
            
    # @filter.command("å½’æ¡£eh")
    async def archive_gallery(self, event: AstrMessageEvent):
        output_config = self.config.get('output', {})
        search_cache_folder = Path(output_config.get('search_cache_folder', 'data/ehentai/searchCache'))
        search_cache_folder.mkdir(exist_ok=True, parents=True)

        try:
            args = self.parse_command(event.message_str)
            if len(args) != 1:
                await event.send(event.plain_result("å‚æ•°é”™è¯¯ï¼Œå½’æ¡£æ“ä½œåªéœ€è¦ä¸€ä¸ªå‚æ•°ï¼ˆç”»å»Šé“¾æ¥æˆ–æœç´¢ç»“æœåºå·ï¼‰"))
                return

            url = await self._resolve_url_from_input(event, args[0])
            if not url:
                return

            pattern = re.compile(r'^https://(e-hentai|exhentai)\.org/g/(\d{7})/([a-f0-9]{10})/?$')
            match = pattern.match(url)
            if not match:
                await event.send(event.plain_result("æ— æ³•è§£æç”»å»Šé“¾æ¥ï¼Œè¯·é‡è¯•..."))
                return

            _, gid, token = match.groups()
            
            await event.send(event.plain_result("æ­£åœ¨è·å–å½’æ¡£é“¾æ¥ï¼Œè¯·ç¨å€™..."))
            
            async with await self._get_session() as session:
                download_url = await self.downloader.get_archive_url(session, gid, token)
                
                if download_url:
                    await event.send(event.plain_result(f"å½’æ¡£é“¾æ¥è·å–æˆåŠŸï¼Œè¯·å°½å¿«ä¸‹è½½ï¼ˆé“¾æ¥ä»…èƒ½è®¿é—®ä¸€æ¬¡ï¼‰ï¼š\n{download_url}"))
                else:
                    await event.send(event.plain_result("å½’æ¡£é“¾æ¥è·å–å¤±è´¥ï¼Œè¯·æ£€æŸ¥è´¦å·æƒé™æˆ–é‡è¯•"))

        except Exception as e:
            logger.exception("å½’æ¡£å¤±è´¥")
            await event.send(event.plain_result(f"å½’æ¡£å¤±è´¥ï¼š{str(e)}"))

    @filter.command("eh")
    async def eh_helper(self, event: AstrMessageEvent):
        help_text = """ğŸ“– EHentai æ’ä»¶ä½¿ç”¨æŒ‡å— (v4.0.9)
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
ğŸ” æœç´¢ä¸ä¸‹è½½
/es <å…³é”®è¯> [é¡µç ]
â€¢ æœç´¢ç”»å»Šï¼Œç»“æœä¸­å›å¤æ•°å­—å¯å¿«é€Ÿä¸‹è½½
â€¢ ç¤ºä¾‹: /es loli
â€¢ ç¤ºä¾‹: /es loli 2

ğŸš€ å¿«é€Ÿä¸‹è½½
â€¢ åœ¨æœç´¢ç»“æœå‡ºç°å 30ç§’å†…ï¼Œç›´æ¥å›å¤åºå· (1-9) å³å¯å¼€å§‹ä¸‹è½½

ğŸ”— é“¾æ¥è§£æ
â€¢ å‘é€ E-Hentai/ExHentai ç”»å»Šé“¾æ¥ï¼Œè‡ªåŠ¨è§£æå¹¶æä¾›ä¸‹è½½é€‰é¡¹

â„¹ï¸ å…¶ä»–
â€¢ /eh <ID> <Token> - é«˜çº§ä¸‹è½½ (ä¸€èˆ¬ç”±æŒ‰é’®æˆ–é“¾æ¥è§¦å‘)
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”"""
        await event.send(event.plain_result(help_text))

    @filter.command("é‡è½½ehé…ç½®")
    async def reload_config(self, event: AstrMessageEvent):
        await event.send(event.plain_result("æ­£åœ¨é‡è½½é…ç½®å‚æ•°"))
        # é…ç½®ç”±æ¡†æ¶ç®¡ç†ï¼Œæ— éœ€æ‰‹åŠ¨é‡è½½
        self.uploader = MessageAdapter(self.config)
        self.downloader = Downloader(self.config, self.uploader, self.parser)
        await event.send(event.plain_result("å·²é‡è½½é…ç½®å‚æ•°"))
    
    # @filter.regex(r"^(?:\[([^\]]+)\]|\(([^\)]+)\))\s*(.*)$")
    # async def search_by_formatted_message(self, event: AstrMessageEvent):
        """
        ç›‘å¬ç‰¹å®šæ ¼å¼çš„æ¶ˆæ¯ï¼Œè‡ªåŠ¨æå–ä½œè€…åå’Œä½œå“åï¼Œå¹¶æ‹¼æ¥ä¸ºæœç´¢å…³é”®è¯è¿›è¡Œæœç´¢ã€‚
        """
        # æ£€æŸ¥æ˜¯å¦å¯ç”¨æ ¼å¼åŒ–æ¶ˆæ¯æœç´¢åŠŸèƒ½
        if not self.config.get("features", {}).get("enable_formatted_message_search", True):
            return 
            
        match = re.search(r"^(?:\[([^\]]+)\]|\(([^\)]+)\))\s*(.*)$", event.message_str)
        if not match:
            return
            
        author = match.group(1) if match.group(1) else match.group(2)
        title = match.group(3).strip()

        # ç§»é™¤ä½œå“åä¸­å¯èƒ½å­˜åœ¨çš„é¢å¤–ä¿¡æ¯ï¼Œä¾‹å¦‚[ä¸­å›½ç¿»è¨³]
        title = re.sub(r'\[[^\]]+\]|\([^\)]+\)', '', title).strip()

        if not author or not title:
            logger.warning(f"æœªèƒ½ä»æ¶ˆæ¯ä¸­æå–æœ‰æ•ˆçš„ä½œè€…æˆ–ä½œå“å: {event.message_str}")
            return

        # å°†ç©ºæ ¼æ›¿æ¢ä¸º+
        search_query = f"{author.replace(' ', '+')}+{title.replace(' ', '+')}"
        
        event.message_str = f"æœeh {search_query}"
        
        await self.search_gallery(event)
        
        return
        
    async def terminate(self):
        pass
